{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CF](https://dataconomy.com/wp-content/uploads/2015/03/Beginners-Guide-Recommender-Systems-Collaborative-Filtering-620x340.jpg)\n",
    "\n",
    "## **Collaborative Filtering**\n",
    "+ **predicting** what **users** will **like** based on their **similarity to other users.**\n",
    "+ **Advantages:** capable of accurately recommending complex items such as movies without requiring an “understanding” of the item itself. \n",
    "+ many  have been used in measuring (**user similarity** or **item similarity**) in **recommender systems.** \n",
    "+ **Task 1**: finding similar animes\n",
    "+ **Task 2**: finding similar users\n",
    "+ **Task 3**: Recommending Animes for a random user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-05-17T06:42:36.147899Z",
     "iopub.status.busy": "2023-05-17T06:42:36.147576Z",
     "iopub.status.idle": "2023-05-17T06:42:36.484553Z",
     "shell.execute_reply": "2023-05-17T06:42:36.482924Z",
     "shell.execute_reply.started": "2023-05-17T06:42:36.147876Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input/anime-recommendation-database-2020'\n",
    "!ls {INPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:42:36.486686Z",
     "iopub.status.busy": "2023-05-17T06:42:36.486369Z",
     "iopub.status.idle": "2023-05-17T06:43:39.526029Z",
     "shell.execute_reply": "2023-05-17T06:43:39.523721Z",
     "shell.execute_reply.started": "2023-05-17T06:42:36.486661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rating_df = pd.read_csv(INPUT_DIR + '/animelist.csv', \n",
    "                        low_memory=False, \n",
    "                        usecols=[\"user_id\", \"anime_id\", \"rating\"]\n",
    "                        #, nrows=90000000\n",
    "                        )\n",
    "rating_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:43:39.52831Z",
     "iopub.status.busy": "2023-05-17T06:43:39.527982Z",
     "iopub.status.idle": "2023-05-17T06:43:43.460077Z",
     "shell.execute_reply": "2023-05-17T06:43:43.459161Z",
     "shell.execute_reply.started": "2023-05-17T06:43:39.528276Z"
    }
   },
   "outputs": [],
   "source": [
    "# User should rate atleast 400 animies\n",
    "n_ratings = rating_df['user_id'].value_counts()\n",
    "rating_df = rating_df[rating_df['user_id'].isin(n_ratings[n_ratings >= 400].index)].copy()\n",
    "len(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:43:43.463883Z",
     "iopub.status.busy": "2023-05-17T06:43:43.462835Z",
     "iopub.status.idle": "2023-05-17T06:44:14.924617Z",
     "shell.execute_reply": "2023-05-17T06:44:14.923626Z",
     "shell.execute_reply.started": "2023-05-17T06:43:43.463848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling BTW (0 , 1.0)\n",
    "min_rating = min(rating_df['rating'])\n",
    "max_rating = max(rating_df['rating'])\n",
    "rating_df['rating'] = rating_df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype(np.float64)\n",
    "\n",
    "AvgRating = np.mean(rating_df['rating'])\n",
    "print('Avg', AvgRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:44:14.926803Z",
     "iopub.status.busy": "2023-05-17T06:44:14.926175Z",
     "iopub.status.idle": "2023-05-17T06:45:22.222693Z",
     "shell.execute_reply": "2023-05-17T06:45:22.220871Z",
     "shell.execute_reply.started": "2023-05-17T06:44:14.926771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing Duplicated Rows\n",
    "duplicates = rating_df.duplicated()\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print('> {} duplicates'.format(duplicates.sum()))\n",
    "    rating_df = rating_df[~duplicates]\n",
    "\n",
    "print('> {} duplicates'.format(rating_df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://content.codecademy.com/programs/code-foundations-path/ds-survey/utilitymatrix.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-05-17T06:45:22.225752Z",
     "iopub.status.busy": "2023-05-17T06:45:22.225269Z",
     "iopub.status.idle": "2023-05-17T06:45:27.824675Z",
     "shell.execute_reply": "2023-05-17T06:45:27.823085Z",
     "shell.execute_reply.started": "2023-05-17T06:45:22.225716Z"
    }
   },
   "outputs": [],
   "source": [
    "g = rating_df.groupby('user_id')['rating'].count()\n",
    "top_users = g.dropna().sort_values(ascending=False)[:20]\n",
    "top_r = rating_df.join(top_users, rsuffix='_r', how='inner', on='user_id')\n",
    "\n",
    "g = rating_df.groupby('anime_id')['rating'].count()\n",
    "top_animes = g.dropna().sort_values(ascending=False)[:20]\n",
    "top_r = top_r.join(top_animes, rsuffix='_r', how='inner', on='anime_id')\n",
    "\n",
    "pd.crosstab(top_r.user_id, top_r.anime_id, top_r.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:45:27.826311Z",
     "iopub.status.busy": "2023-05-17T06:45:27.826018Z",
     "iopub.status.idle": "2023-05-17T06:45:42.219499Z",
     "shell.execute_reply": "2023-05-17T06:45:42.218539Z",
     "shell.execute_reply.started": "2023-05-17T06:45:27.826288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "user_ids = rating_df[\"user_id\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "user_encoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "rating_df[\"user\"] = rating_df[\"user_id\"].map(user2user_encoded)\n",
    "n_users = len(user2user_encoded)\n",
    "\n",
    "anime_ids = rating_df[\"anime_id\"].unique().tolist()\n",
    "anime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}\n",
    "anime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}\n",
    "rating_df[\"anime\"] = rating_df[\"anime_id\"].map(anime2anime_encoded)\n",
    "n_animes = len(anime2anime_encoded)\n",
    "\n",
    "print(\"Num of users: {}, Num of animes: {}\".format(n_users, n_animes))\n",
    "print(\"Min rating: {}, Max rating: {}\".format(min(rating_df['rating']), max(rating_df['rating'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:45:42.220955Z",
     "iopub.status.busy": "2023-05-17T06:45:42.220717Z",
     "iopub.status.idle": "2023-05-17T06:46:01.606834Z",
     "shell.execute_reply": "2023-05-17T06:46:01.604978Z",
     "shell.execute_reply.started": "2023-05-17T06:45:42.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "rating_df = rating_df.sample(frac=1, random_state=73)\n",
    "\n",
    "X = rating_df[['user', 'anime']].values\n",
    "y = rating_df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:01.608837Z",
     "iopub.status.busy": "2023-05-17T06:46:01.608449Z",
     "iopub.status.idle": "2023-05-17T06:46:01.61529Z",
     "shell.execute_reply": "2023-05-17T06:46:01.614663Z",
     "shell.execute_reply.started": "2023-05-17T06:46:01.608815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "test_set_size = 10000 #10k for test set\n",
    "train_indices = rating_df.shape[0] - test_set_size \n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    X[:train_indices],\n",
    "    X[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")\n",
    "\n",
    "print('> Train set ratings: {}'.format(len(y_train)))\n",
    "print('> Test set ratings: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:01.618968Z",
     "iopub.status.busy": "2023-05-17T06:46:01.618435Z",
     "iopub.status.idle": "2023-05-17T06:46:01.631906Z",
     "shell.execute_reply": "2023-05-17T06:46:01.63122Z",
     "shell.execute_reply.started": "2023-05-17T06:46:01.618942Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_array = [X_train[:, 0], X_train[:, 1]]\n",
    "X_test_array = [X_test[:, 0], X_test[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:01.633368Z",
     "iopub.status.busy": "2023-05-17T06:46:01.632974Z",
     "iopub.status.idle": "2023-05-17T06:46:15.216707Z",
     "shell.execute_reply": "2023-05-17T06:46:15.214879Z",
     "shell.execute_reply.started": "2023-05-17T06:46:01.633345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accelerator check\n",
    "import tensorflow as tf\n",
    "\n",
    "TPU_INIT = True\n",
    "\n",
    "if TPU_INIT:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    !nvidia-smi\n",
    "    \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:15.219293Z",
     "iopub.status.busy": "2023-05-17T06:46:15.218454Z",
     "iopub.status.idle": "2023-05-17T06:46:15.227466Z",
     "shell.execute_reply": "2023-05-17T06:46:15.22557Z",
     "shell.execute_reply.started": "2023-05-17T06:46:15.219256Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:15.22987Z",
     "iopub.status.busy": "2023-05-17T06:46:15.229235Z",
     "iopub.status.idle": "2023-05-17T06:46:17.244581Z",
     "shell.execute_reply": "2023-05-17T06:46:17.243113Z",
     "shell.execute_reply.started": "2023-05-17T06:46:15.229839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten\n",
    "\n",
    "def RecommenderNet():\n",
    "    embedding_size = 128\n",
    "    \n",
    "    user = Input(name = 'user', shape = [1])\n",
    "    user_embedding = Embedding(name = 'user_embedding',\n",
    "                       input_dim = n_users, \n",
    "                       output_dim = embedding_size)(user)\n",
    "    \n",
    "    anime = Input(name = 'anime', shape = [1])\n",
    "    anime_embedding = Embedding(name = 'anime_embedding',\n",
    "                       input_dim = n_animes, \n",
    "                       output_dim = embedding_size)(anime)\n",
    "    \n",
    "    #x = Concatenate()([user_embedding, anime_embedding])\n",
    "    x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])\n",
    "    x = Flatten()(x)\n",
    "        \n",
    "    x = Dense(1, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[user, anime], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='Adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "if TPU_INIT:    \n",
    "    with tpu_strategy.scope():\n",
    "        model = RecommenderNet()\n",
    "else:\n",
    "    model = RecommenderNet()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:17.246226Z",
     "iopub.status.busy": "2023-05-17T06:46:17.245955Z",
     "iopub.status.idle": "2023-05-17T06:46:17.25896Z",
     "shell.execute_reply": "2023-05-17T06:46:17.25738Z",
     "shell.execute_reply.started": "2023-05-17T06:46:17.246202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "start_lr = 0.00001\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.00005\n",
    "batch_size = 10000\n",
    "\n",
    "if TPU_INIT:\n",
    "    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n",
    "    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "rampup_epochs = 5\n",
    "sustain_epochs = 0\n",
    "exp_decay = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        return max_lr\n",
    "    else:\n",
    "        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n",
    "\n",
    "checkpoint_filepath = './weights.h5'\n",
    "\n",
    "model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3, monitor='val_loss', \n",
    "                               mode='min', restore_best_weights=True)\n",
    "\n",
    "my_callbacks = [\n",
    "    model_checkpoints,\n",
    "    lr_callback,\n",
    "    early_stopping,   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T06:46:17.260922Z",
     "iopub.status.busy": "2023-05-17T06:46:17.260619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "history = model.fit(\n",
    "    x=X_train_array,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_array, y_test),\n",
    "    callbacks=my_callbacks\n",
    ")\n",
    "\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history[\"loss\"][0:-2])\n",
    "plt.plot(history.history[\"val_loss\"][0:-2])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extracting weights from model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
    "    return weights\n",
    "\n",
    "anime_weights = extract_weights('anime_embedding', model)\n",
    "user_weights = extract_weights('user_embedding', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **anime meta data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_DIR + '/anime.csv', low_memory=True)\n",
    "df = df.replace(\"Unknown\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Names\n",
    "def getAnimeName(anime_id):\n",
    "    try:\n",
    "        name = df[df.anime_id == anime_id].eng_version.values[0]\n",
    "        if name is np.nan:\n",
    "            name = df[df.anime_id == anime_id].Name.values[0]\n",
    "    except:\n",
    "        print('error')\n",
    "    \n",
    "    return name\n",
    "\n",
    "df['anime_id'] = df['MAL_ID']\n",
    "df[\"eng_version\"] = df['English name']\n",
    "df['eng_version'] = df.anime_id.apply(lambda x: getAnimeName(x))\n",
    "\n",
    "df.sort_values(by=['Score'], \n",
    "               inplace=True,\n",
    "               ascending=False, \n",
    "               kind='quicksort',\n",
    "               na_position='last')\n",
    "\n",
    "df = df[[\"anime_id\", \"eng_version\", \n",
    "         \"Score\", \"Genres\", \"Episodes\", \n",
    "         \"Type\", \"Premiered\", \"Members\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnimeFrame(anime):\n",
    "    if isinstance(anime, int):\n",
    "        return df[df.anime_id == anime]\n",
    "    if isinstance(anime, str):\n",
    "        return df[df.eng_version == anime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sypnopsis data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"MAL_ID\", \"Name\", \"Genres\", \"sypnopsis\"]\n",
    "sypnopsis_df = pd.read_csv(INPUT_DIR + '/anime_with_synopsis.csv', usecols=cols)\n",
    "\n",
    "def getSypnopsis(anime):\n",
    "    if isinstance(anime, int):\n",
    "        return sypnopsis_df[sypnopsis_df.MAL_ID == anime].sypnopsis.values[0]\n",
    "    if isinstance(anime, str):\n",
    "        return sypnopsis_df[sypnopsis_df.Name == anime].sypnopsis.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1**: Finding Similar Animes (Item Based Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.reset_option('all')\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "def find_similar_animes(name, n=10, return_dist=False, neg=False):\n",
    "    try:\n",
    "        index = getAnimeFrame(name).anime_id.values[0]\n",
    "        encoded_index = anime2anime_encoded.get(index)\n",
    "        weights = anime_weights\n",
    "        \n",
    "        dists = np.dot(weights, weights[encoded_index])\n",
    "        sorted_dists = np.argsort(dists)\n",
    "        \n",
    "        n = n + 1            \n",
    "        \n",
    "        if neg:\n",
    "            closest = sorted_dists[:n]\n",
    "        else:\n",
    "            closest = sorted_dists[-n:]\n",
    "\n",
    "        print('animes closest to {}'.format(name))\n",
    "\n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "        \n",
    "        rindex = df\n",
    "\n",
    "        SimilarityArr = []\n",
    "\n",
    "        for close in closest:\n",
    "            decoded_id = anime_encoded2anime.get(close)\n",
    "            sypnopsis = getSypnopsis(decoded_id)\n",
    "            anime_frame = getAnimeFrame(decoded_id)\n",
    "            \n",
    "            anime_name = anime_frame.eng_version.values[0]\n",
    "            genre = anime_frame.Genres.values[0]\n",
    "            similarity = dists[close]\n",
    "            SimilarityArr.append({\"anime_id\": decoded_id, \"name\": anime_name,\n",
    "                                  \"similarity\": similarity,\"genre\": genre,\n",
    "                                  'sypnopsis': sypnopsis})\n",
    "\n",
    "        Frame = pd.DataFrame(SimilarityArr).sort_values(by=\"similarity\", ascending=False)\n",
    "        return Frame[Frame.anime_id != index].drop(['anime_id'], axis=1)\n",
    "\n",
    "    except:\n",
    "        print('{}!, Not Found in Anime list'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **these animes are my fav**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.pinimg.com/originals/1f/cb/2a/1fcb2af4376fe78b6d82197bd1fdbff6.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_animes('Dragon Ball Z', n=5, neg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://64.media.tumblr.com/1b942774dc6d4240cfbb3da22d99a681/tumblr_phsucvmeDT1sivxmj_500.gifv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_animes('Your Name.', n=5, neg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.pinimg.com/originals/26/fd/49/26fd49fa54b204fbaf6301efefd53ae2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_animes('Sword Art Online', n=5, neg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://steamuserimages-a.akamaihd.net/ugc/993512070845192516/C18040A95DB14DD58438DDDEBF721BA8ABAD0E84/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_animes('Black Clover', n=5, neg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_animes('Death Note', n=5, neg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**: Finding Similar Users (User Based Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> picking up random user')\n",
    "\n",
    "ratings_per_user = rating_df.groupby('user_id').size()\n",
    "random_user = ratings_per_user[ratings_per_user < 500].sample(1, random_state=None).index[0]\n",
    "print('> user_id:', random_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.reset_option('all')\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "def find_similar_users(item_input, n=10,return_dist=False, neg=False):\n",
    "    try:\n",
    "        index = item_input\n",
    "        encoded_index = user2user_encoded.get(index)\n",
    "        weights = user_weights\n",
    "    \n",
    "        dists = np.dot(weights, weights[encoded_index])\n",
    "        sorted_dists = np.argsort(dists)\n",
    "        \n",
    "        n = n + 1\n",
    "        \n",
    "        if neg:\n",
    "            closest = sorted_dists[:n]\n",
    "        else:\n",
    "            closest = sorted_dists[-n:]\n",
    "\n",
    "        print('> users similar to #{}'.format(item_input))\n",
    "\n",
    "        if return_dist:\n",
    "            return dists, closest\n",
    "        \n",
    "        rindex = df\n",
    "        SimilarityArr = []\n",
    "        \n",
    "        for close in closest:\n",
    "            similarity = dists[close]\n",
    "\n",
    "            if isinstance(item_input, int):\n",
    "                decoded_id = user_encoded2user.get(close)\n",
    "                SimilarityArr.append({\"similar_users\": decoded_id, \n",
    "                                      \"similarity\": similarity})\n",
    "\n",
    "        Frame = pd.DataFrame(SimilarityArr).sort_values(by=\"similarity\", \n",
    "                                                        ascending=False)\n",
    "        \n",
    "        return Frame\n",
    "    \n",
    "    except:\n",
    "        print('{}!, Not Found in User list'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_users = find_similar_users(int(random_user), \n",
    "                                   n=5, \n",
    "                                   neg=False)\n",
    "\n",
    "similar_users = similar_users[similar_users.similarity > 0.4]\n",
    "similar_users = similar_users[similar_users.similar_users != random_user]\n",
    "similar_users.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **User preferences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showWordCloud(all_genres):\n",
    "    genres_cloud = WordCloud(width=700, height=400, \n",
    "                             background_color='white', \n",
    "                             colormap='gnuplot').generate_from_frequencies(all_genres)\n",
    "    \n",
    "    plt.figure(figsize=(10,8)) \n",
    "    plt.imshow(genres_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def getFavGenre(frame, plot=False):\n",
    "        frame.dropna(inplace=False)\n",
    "        all_genres = defaultdict(int)\n",
    "        \n",
    "        genres_list = []\n",
    "        for genres in frame['Genres']:\n",
    "            if isinstance(genres, str):\n",
    "                for genre in genres.split(','):\n",
    "                    genres_list.append(genre)\n",
    "                    all_genres[genre.strip()] += 1    \n",
    "        if plot:\n",
    "            showWordCloud(all_genres)\n",
    "        \n",
    "        return genres_list\n",
    "\n",
    "    \n",
    "def get_user_preferences(user_id, plot=False, verbose=0):\n",
    "    animes_watched_by_user = rating_df[rating_df.user_id==user_id]\n",
    "    user_rating_percentile = np.percentile(animes_watched_by_user.rating, 75)\n",
    "    animes_watched_by_user = animes_watched_by_user[animes_watched_by_user.rating >= user_rating_percentile]\n",
    "    top_animes_user = (\n",
    "        animes_watched_by_user.sort_values(by=\"rating\", ascending=False)#.head(10)\n",
    "        .anime_id.values\n",
    "    )\n",
    "    \n",
    "    anime_df_rows = df[df[\"anime_id\"].isin(top_animes_user)]\n",
    "    anime_df_rows = anime_df_rows[[\"eng_version\", \"Genres\"]]\n",
    "    \n",
    "    if verbose != 0:\n",
    "        print(\"> User #{} has rated {} movies (avg. rating = {:.1f})\".format(\n",
    "          user_id, len(animes_watched_by_user),\n",
    "          animes_watched_by_user['rating'].mean(),\n",
    "        ))\n",
    "    \n",
    "        print('> preferred genres')\n",
    "    \n",
    "    if plot:\n",
    "        getFavGenre(anime_df_rows, plot)\n",
    "        \n",
    "    return anime_df_rows#.eng_version.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref = get_user_preferences(random_user, plot=True, verbose=1)\n",
    "print('> animes highly rated by this user')\n",
    "\n",
    "pd.DataFrame(user_pref).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3**: **Recommending** animes for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_animes(similar_users, n=10):\n",
    "    recommended_animes = []\n",
    "    anime_list = []\n",
    "    \n",
    "    for user_id in similar_users.similar_users.values:\n",
    "        pref_list = get_user_preferences(int(user_id), verbose=0)\n",
    "        pref_list = pref_list[~ pref_list.eng_version.isin(user_pref.eng_version.values)]\n",
    "        anime_list.append(pref_list.eng_version.values)\n",
    "        \n",
    "    anime_list = pd.DataFrame(anime_list)\n",
    "    sorted_list = pd.DataFrame(pd.Series(anime_list.values.ravel()).value_counts()).head(n)\n",
    "    \n",
    "    for i, anime_name in enumerate(sorted_list.index):        \n",
    "        n_user_pref = sorted_list[sorted_list.index == anime_name].values[0][0]\n",
    "        if isinstance(anime_name, str):\n",
    "            try:\n",
    "                frame = getAnimeFrame(anime_name)\n",
    "                anime_id = frame.anime_id.values[0]\n",
    "                genre = frame.Genres.values[0]\n",
    "                sypnopsis = getSypnopsis(int(anime_id))\n",
    "                recommended_animes.append({#\"anime_id\": anime_id ,\n",
    "                                            \"n\": n_user_pref,\n",
    "                                            \"anime_name\": anime_name, \n",
    "                                            \"Genres\": genre, \n",
    "                                            \"sypnopsis\": sypnopsis})\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(recommended_animes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_animes = get_recommended_animes(similar_users, n=10)\n",
    "getFavGenre(recommended_animes, plot=True)\n",
    "\n",
    "print('\\n> Top recommendations for user: {}'.format(random_user))\n",
    "recommended_animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ranking based Recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing recommendations for user: {}\".format(random_user))\n",
    "print(\"===\" * 25)\n",
    "\n",
    "animes_watched_by_user = rating_df[rating_df.user_id==random_user]\n",
    "anime_not_watched_df = df[\n",
    "    ~df[\"anime_id\"].isin(animes_watched_by_user.anime_id.values)\n",
    "]\n",
    "\n",
    "anime_not_watched = list(\n",
    "    set(anime_not_watched_df['anime_id']).intersection(set(anime2anime_encoded.keys()))\n",
    ")\n",
    "\n",
    "anime_not_watched = [[anime2anime_encoded.get(x)] for x in anime_not_watched]\n",
    "\n",
    "user_encoder = user2user_encoded.get(random_user)\n",
    "\n",
    "user_anime_array = np.hstack(\n",
    "    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)\n",
    ")\n",
    "\n",
    "user_anime_array = [user_anime_array[:, 0], user_anime_array[:, 1]]\n",
    "ratings = model.predict(user_anime_array).flatten()\n",
    "\n",
    "top_ratings_indices = (-ratings).argsort()[:10]\n",
    "\n",
    "recommended_anime_ids = [\n",
    "    anime_encoded2anime.get(anime_not_watched[x][0]) for x in top_ratings_indices\n",
    "]\n",
    "\n",
    "Results = []\n",
    "top_rated_ids = []\n",
    "\n",
    "for index, anime_id in enumerate(anime_not_watched):\n",
    "    rating = ratings[index]\n",
    "    id_ = anime_encoded2anime.get(anime_id[0])\n",
    "    \n",
    "    if id_ in recommended_anime_ids:\n",
    "        top_rated_ids.append(id_)\n",
    "        try:\n",
    "            condition = (df.anime_id == id_)\n",
    "            name = df[condition]['eng_version'].values[0]\n",
    "            genre = df[condition].Genres.values[0]\n",
    "            score = df[condition].Score.values[0]\n",
    "            sypnopsis = getSypnopsis(int(id_))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        Results.append({#\"anime_id\": id_, \n",
    "                        \"name\": name, \n",
    "                        \"pred_rating\": rating,\n",
    "                        \"genre\": genre, \n",
    "                        'sypnopsis': sypnopsis})\n",
    "\n",
    "print(\"---\" * 25)\n",
    "print(\"> Top 10 anime recommendations\")\n",
    "print(\"---\" * 25)\n",
    "\n",
    "\n",
    "Results = pd.DataFrame(Results).sort_values(by='pred_rating', ascending=False)\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10008\\59292263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'anime_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileLink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mFileLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'./anime_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save('anime_model.h5')\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'./anime_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
